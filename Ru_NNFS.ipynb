{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLXpitOuYbcYEcMCMyqeX5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedantg-1311/Ru-Bioactivity-Classification/blob/main/Ru_NNFS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application of Neural Networks to Classify Transition Metal Complexes of Ruthenium"
      ],
      "metadata": {
        "id": "Vcovjs9te203"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing libraries and dataset"
      ],
      "metadata": {
        "id": "xFWLn1ZzfQej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BPXC84GceSWZ",
        "outputId": "fd546a75-436e-40d3-8951-02ae852b554e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (10.4.0)\n",
            "Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TheFreiLab/RutheniumML.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UPuhk-sHehBT",
        "outputId": "da5af670-b6ba-4836-ab82-005f62ee7caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RutheniumML'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 31 (delta 4), reused 26 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (31/31), 1.49 MiB | 6.67 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, DataStructs, Draw\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "9nWLDuwegNdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ],
      "metadata": {
        "id": "5CVMfGh-f9Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arenes = pd.read_csv('RutheniumML/data/tested_arenes.csv', dtype={'ID': str, 'SMILES': str})\n",
        "amines = pd.read_csv('RutheniumML/data/tested_amines.csv', dtype={'ID': str, 'SMILES': str})\n",
        "aldehydes = pd.read_csv('RutheniumML/data/tested_aldehydes.csv', dtype={'ID': str, 'SMILES': str})\n",
        "actives = pd.read_csv('RutheniumML/data/actives.csv', dtype={'ID': str, 'MIC': float, 'CC50': float, 'HC10': float})\n",
        "actives['IsActive'] = 1\n",
        "\n",
        "arenes['MOL'] = arenes.SMILES.apply(Chem.MolFromSmiles)\n",
        "amines['MOL'] = amines.SMILES.apply(Chem.MolFromSmiles)\n",
        "aldehydes['MOL'] = aldehydes.SMILES.apply(Chem.MolFromSmiles)\n",
        "\n",
        "def calc_ecfp4(mol):\n",
        "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=512)\n",
        "    arr = np.zeros((0,), dtype=np.int8)\n",
        "    DataStructs.ConvertToNumpyArray(fp,arr)\n",
        "    return arr\n",
        "\n",
        "arenes['ECFP4'] = arenes.MOL.apply(calc_ecfp4)\n",
        "amines['ECFP4'] = amines.MOL.apply(calc_ecfp4)\n",
        "aldehydes['ECFP4'] = aldehydes.MOL.apply(calc_ecfp4)\n",
        "\n",
        "id = []\n",
        "smiles = []\n",
        "ecfp4 = []\n",
        "\n",
        "for i in range(len(arenes)):\n",
        "    for j in range(len(amines)):\n",
        "        for k in range(len(aldehydes)):\n",
        "            id.append(f'{arenes.ID.iloc[i]}{amines.ID.iloc[j]}{aldehydes.ID.iloc[k]}')\n",
        "            smiles.append(f'{arenes.SMILES.iloc[i]}.{amines.SMILES.iloc[j]}.{aldehydes.SMILES.iloc[k]}')\n",
        "            ecfp4.append(np.sum([arenes.ECFP4.iloc[i], amines.ECFP4.iloc[j], aldehydes.ECFP4.iloc[k]], axis=0)) # ECFPs are summed\n",
        "\n",
        "df_dataset = pd.DataFrame(list(zip(id, smiles, ecfp4)), columns=['ID', 'SMILES', 'ECFP4'])\n",
        "\n",
        "df_train = df_dataset.copy()\n",
        "actives_id = actives.ID.values.tolist()\n",
        "\n",
        "def IsActive(id, id_list):\n",
        "    if id in id_list:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df_train['IsActive'] = df_train.ID.apply(lambda x: IsActive(x, actives_id))\n",
        "\n",
        "X = np.array(df_train.ECFP4.values.tolist())\n",
        "y = np.array(df_train.IsActive.values.tolist())\n",
        "\n",
        "df_actives_only = df_train.loc[df_train['IsActive'] == True]\n",
        "df_inactives_only = df_train.loc[df_train['IsActive'] == False]\n",
        "X_actives_only = np.array(df_actives_only.ECFP4.values.tolist())\n",
        "X_inactives_only = np.array(df_inactives_only.ECFP4.values.tolist())\n",
        "y_actives_only = np.array(df_actives_only.IsActive.values.tolist())\n",
        "y_inactives_only = np.array(df_inactives_only.IsActive.values.tolist())\n",
        "\n",
        "X_actives_train, X_actives_test, y_actives_train, y_actives_test = train_test_split(X_actives_only, y_actives_only, test_size=0.20, random_state=1)\n",
        "X_inactives_train, X_inactives_test, y_inactives_train, y_inactives_test = train_test_split(X_inactives_only, y_inactives_only, test_size=0.20, random_state=1)\n",
        "\n",
        "y_train_final = np.append(y_actives_train, y_inactives_train)\n",
        "X_train_final = np.append(X_actives_train, X_inactives_train, axis=0)"
      ],
      "metadata": {
        "id": "DAQq_hnje1Uy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d91caa89-fbed-4035-9124-dff411395c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n",
            "[08:15:31] DEPRECATION WARNING: please use MorganGenerator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Architecture"
      ],
      "metadata": {
        "id": "JkzpBSsUjdPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_Dense:\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dinputs = dvalues.copy()\n",
        "    self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self, inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) #????????????????????????????????????????????\n",
        "    probabilities = exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
        "    self.output = probabilities\n",
        "\n",
        "class Loss:\n",
        "  def calculate(self, output, y):\n",
        "    sample_losses = self.forward(output, y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped  = np.clip(y_pred, 1e-7, 1-1e-7)\n",
        "\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true] #???????????????????????????????\n",
        "\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
        "\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "  def backward(self, dvalues, y_true):\n",
        "    samples = len(dvalues)\n",
        "    labels = len(dvalues[0])\n",
        "\n",
        "    if len(y_true.shape) == 1:\n",
        "      y_true = np.eye(labels)[y_true] #?????????????????\n",
        "\n",
        "    self.dinputs = -y_true / dvalues\n",
        "    self.dinputs = self.dinputs / samples\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
        "  def __init__(self):\n",
        "    self.activation = Activation_Softmax()\n",
        "    self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "  def forward(self, inputs, y_true):\n",
        "    self.activation.forward(inputs)\n",
        "    self.output = self.activation.output\n",
        "    return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "  def backward(self, dvalues, y_true):\n",
        "    samples = len(dvalues)\n",
        "    if len(y_true.shape) == 2:\n",
        "      y_true = np.argmax(y_true, axis=1)\n",
        "    self.dinputs = dvalues.copy()\n",
        "    self.dinputs[range(samples), y_true] -= 1\n",
        "    self.dinputs = self.dinputs / samples\n",
        "\n",
        "class Optimizer_Adam:\n",
        "  def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self, layer):\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "    layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "    bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "    layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ],
      "metadata": {
        "id": "m3FfioKJjy3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "jcVUfy_dFQcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Layer_Dense(512, 400)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense(400, 300)\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_Adam(learning_rate=0.02, decay=1e-5)\n",
        "\n",
        "for epoch in range(1001):\n",
        "  dense1.forward(X_train_final)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  loss = loss_activation.forward(dense2.output, y_train_final)\n",
        "\n",
        "  predictions = np.argmax(loss_activation.output, axis=1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  accuracy = np.mean(predictions == y_train_final)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, ' +\n",
        "          f'acc: {accuracy:.3f}, ' +\n",
        "          f'loss: {loss:.3f}, ' +\n",
        "          f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "  loss_activation.backward(loss_activation.output, y_train_final)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfSwX9Vvnk1_",
        "outputId": "0bffe73a-4197-41b0-9d66-f986c3bc056f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.000, loss: 5.703, lr: 0.02\n",
            "epoch: 100, acc: 1.000, loss: 0.006, lr: 0.01998021958261321\n",
            "epoch: 200, acc: 1.000, loss: 0.001, lr: 0.019960279044701046\n",
            "epoch: 300, acc: 1.000, loss: 0.000, lr: 0.019940378268975763\n",
            "epoch: 400, acc: 1.000, loss: 0.000, lr: 0.01992051713662487\n",
            "epoch: 500, acc: 1.000, loss: 0.000, lr: 0.01990069552930875\n",
            "epoch: 600, acc: 1.000, loss: 0.000, lr: 0.019880913329158343\n",
            "epoch: 700, acc: 1.000, loss: 0.000, lr: 0.019861170418772778\n",
            "epoch: 800, acc: 1.000, loss: 0.000, lr: 0.019841466681217078\n",
            "epoch: 900, acc: 1.000, loss: 0.000, lr: 0.01982180200001982\n",
            "epoch: 1000, acc: 1.000, loss: 0.000, lr: 0.019802176259170884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "KP2soqowtpXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_final = np.append(X_actives_test, X_inactives_test, axis=0)\n",
        "y_test_final = np.append(y_actives_test, y_inactives_test)\n",
        "\n",
        "dense1.forward(X_test_final)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "loss = loss_activation.forward(dense2.output, y_test_final)\n",
        "predictions = np.argmax(loss_activation.output, axis=1)\n",
        "if len(y_test_final.shape) == 2:\n",
        " y_test = np.argmax(y_test_final, axis=1)\n",
        "accuracy = np.mean(predictions == y_test_final)\n",
        "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPZ2r3Kws2nC",
        "outputId": "e80a27c5-1680-4693-b88e-2a5b7e26936c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation, acc: 0.949, loss: 0.413\n"
          ]
        }
      ]
    }
  ]
}